{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c478414c",
   "metadata": {},
   "source": [
    "## Applied ML for baseball:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2f74e5",
   "metadata": {},
   "source": [
    "#### By Thomas Maxence Franco \n",
    "Submitted to the Faculty of Science in partial fulfillment of the requirements for the degree of \n",
    "#### Master of Modeling for Science and Engineering \n",
    "at the \n",
    "#### UNIVERSITAT AUTÒNOMA DE BARCELONA \n",
    "Directed by \n",
    "Tomás Manuel Margalef Burrull\n",
    "July 2024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0215f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d48c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:\\\\Users\\\\mfran\\\\OneDrive - UAB\\\\Masters\\\\Thesis\\\\Batting\\\\tables\\\\tradfinalbat.csv\"\n",
    "df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3d6560",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dd1004",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2139f9",
   "metadata": {},
   "source": [
    "Null values where handled in a previous step manually. The small amount of null values was due to players not playing through a season because of injury, but that would damage both of the season as we need last season's data to fill the second to last. I had to find each player's stats in order to not lose any data. This can be handled differently to optimize time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59dc45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.rename(columns={'date':'year'})\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df801a7d",
   "metadata": {},
   "source": [
    "#### The Shohei Ohtani case. \n",
    "\n",
    "The japanese superstar signed a record 700M-10year deal with the LA Dodgers this season (2024). The contract itself has very special characteristics that make it unique. First, he is the first player since Babe Ruth to play both pitcher and batter at an elite level. The Dodgers are essentially getting two players extremely talented players in one, except for this year that he won't be able to pitch as he is recovering from a UCL injury. The record AAV for a pitcher excluding Ohtani is 43.3M for Justin Verlander and Max Scherzer. For a batter is 40M per season for Aaron Judge signed last year. \n",
    "\n",
    "But, Ohtani is not getting paid 70M dollars per year. The Dodgers deffered 680 million to be paid starting in 2034, when the contract expires. Ohtani is receiving 2M per season and will get 68 every year starting in 10 years from now. As explained in the paper, today's money won't be worth the same in 10 years. He is not getting 700 million dollars in 2024, he will be getting way less. \n",
    "\n",
    "We could solve this in two ways: Take Ohtani's accumulated WAR from the last 3 years (28.3) and divide it into two: Offensive WAR (14.3) and Pitching WAR (14.2) and calculate how much the Dodgers are paying Ohtani according to his different abilities, which will be approximately 35M for each. Right up there with what the top players at each position have received. \n",
    "\n",
    "\n",
    "The next proposed solution is to take the the annual competitive balance tax (CBT, or luxury tax) figure for the Dodgers, which is approximately 46.6M and use it as a definitive number for both pitching and batting. This estimation to what the Dodgers will be paying him taking interest rates into account. In average MLB salaries have increased 3.5% per year which would make Ohtani's 70 million, 50 million in 2024. \n",
    "\n",
    "For this work I will use the first solution as I see it more fitting for both the pitching models and the batting models. \n",
    "\n",
    "I had already converted the 28.5 WAR to 14.3 in the Data Preprocessing step. \n",
    "\n",
    "Now I will convert the salary and AAV to match its position valuation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dd35c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shohei_index = df2.index[df2['Name'] == 'Shohei Ohtani'].tolist()[0]\n",
    "df2.loc[shohei_index, 'salary'] *= 0.501754386\n",
    "df2.loc[shohei_index, 'AAV'] *= 0.501754386"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e7e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6591c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25702342",
   "metadata": {},
   "source": [
    "### Interest Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceed3b8f",
   "metadata": {},
   "source": [
    "As mentioned before in the Ohtani case, money is not worth the same every year. This will convert every contract to its 2024 value. The coefficients have been calculated previously taking into account the entire salary mass in MLB and the change per year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a4ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[df2['year'] == 2024, 'salary'] *= 1\n",
    "df2.loc[df2['year'] == 2023, 'salary'] *= 0.994232329\n",
    "df2.loc[df2['year'] == 2022, 'salary'] *= 1.097132508\n",
    "df2.loc[df2['year'] == 2020, 'salary'] *= 1.187629677\n",
    "df2.loc[df2['year'] == 2019, 'salary'] *= 1.188733275\n",
    "df2.loc[df2['year'] == 2018, 'salary'] *= 1.183309539\n",
    "df2.loc[df2['year'] == 2017, 'salary'] *= 1.171102114\n",
    "df2.loc[df2['year'] == 2016, 'salary'] *= 1.231297408\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7654a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[df2['year'] == 2024, 'AAV'] *= 1\n",
    "df2.loc[df2['year'] == 2023, 'AAV'] *= 0.994232329\n",
    "df2.loc[df2['year'] == 2022, 'AAV'] *= 1.097132508\n",
    "df2.loc[df2['year'] == 2020, 'AAV'] *= 1.187629677\n",
    "df2.loc[df2['year'] == 2019, 'AAV'] *= 1.188733275\n",
    "df2.loc[df2['year'] == 2018, 'AAV'] *= 1.183309539\n",
    "df2.loc[df2['year'] == 2017, 'AAV'] *= 1.171102114\n",
    "df2.loc[df2['year'] == 2016, 'AAV'] *= 1.231297408\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aa1dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cc3cc6",
   "metadata": {},
   "source": [
    "#### Drop minor league players"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b79488",
   "metadata": {},
   "source": [
    "Minor league contracts have always the same AAV with some rare exceptions. Having these contracts won't help us with trying to predict a value we know already. The objective is to predict which players are worth major league contracts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e560059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "677-(df2['minor_league'] == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a72821",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2[df2['minor_league'] != 1].copy()\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61218d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df3['minor_league'] == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d728d699",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df3[\"minor_league\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eeac4b",
   "metadata": {},
   "source": [
    "#### Categorizing team names. \n",
    "\n",
    "\n",
    "This could help to know if certain teams over pay for free agents or those who spend relatively cheap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963f36aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"team_code\"]=df3[\"new_team\"].astype(\"category\").cat.codes.copy()\n",
    "df3[\"prev_team_code\"]=df3[\"former_team\"].astype(\"category\").cat.codes.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821d19d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a673ccc",
   "metadata": {},
   "source": [
    "#### SB and CS in a single column, as a percentage SB%. \n",
    "\n",
    "This will make the running value more understandable instead of having two variables for the same thing. \n",
    "\n",
    "A runner can have 20SB which is a lot, but could also have 10SB. And while this just shows a big amount of stealing attempts, its not showing us the efficiency of the runner in a clear way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d3061",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"SB_success\"] = (df3[\"SB\"] / (df3[\"SB\"] + df3[\"CS\"]) * 100).fillna(0)\n",
    "df3[\"SB_success_2\"] = (df3[\"SB_2\"] / (df3[\"SB_2\"] + df3[\"CS_2\"]) * 100).fillna(0)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bbc815",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full=df3.copy()\n",
    "df3=df3.copy()\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e26a48",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c6ef87",
   "metadata": {},
   "source": [
    "Our goal is to predict AAV and nothing else. Not contract years or the final accumulated salary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c370139c",
   "metadata": {},
   "source": [
    "Drop salary and contract years as we only need AAV. \n",
    "\n",
    "MLBAMID is repetitive when we have PlayerId.\n",
    "New_team, former_team have already been used to know if the player stayed in the same team after signing.\n",
    "\n",
    "Year wont be need anymore as we have converted all the values to the actual one.\n",
    "\n",
    "WAR3 will be used until the Advanced Statistics part, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63db122",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_columns = ['salary', 'contract_years', 'MLBAMID', 'new_team', 'former_team', 'WAR3', 'SB','SB_2','CS','CS_2']\n",
    "selected_columns = df3.columns[~df3.columns.isin(removed_columns)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a33627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c4add2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.drop(columns=removed_columns).copy()\n",
    "\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09282479",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.select_dtypes(include=['number']).corr().style.background_gradient(\"coolwarm\", vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272a0229",
   "metadata": {},
   "source": [
    "We can see K% in both years doesn't have a significant correlation with AAV. \n",
    "\n",
    "The same goes for BB%. BB%'s correlation is higher, but it is very highly correlated with OBP, and that makes sense as both are calculation for essentialy the same thing. OBP has a much higher correlation with AAV, so dropping BB% for both years wont impact our model. \n",
    "\n",
    "Our created variable 'Catcher' seems to have a very small correlation to AAV, but higher with some others. I chose to keep it.\n",
    "\n",
    "I can't say the same thing for 'stayed_same_team'. Players seem to have accepted a very slight paycut when staying in the same team. This can be attributed, as pointed out by Libsch (2018) to players familiarity with the city, and an already established position. Basically they gave up very little money for comfort. I chose to keep it in the mean time. \n",
    "\n",
    "CS AND SB will be kept for now. \n",
    "\n",
    "Finally, from Yrs / career_games / Age, the one that shows a bigger correlation with AAV is age. It doesn't make sense to keep all three, just AGE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36556ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['BB%', 'BB%_2', 'Yrs', 'career_games', 'K%', 'K%_2', 'team_code', 'prev_team_code']\n",
    "\n",
    "df5 = df4.drop(columns=columns_to_drop)\n",
    "\n",
    "df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879b8b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873796f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_columns = ['catcher', 'stayed_same_team']\n",
    "df5[boolean_columns] = df5[boolean_columns].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ab5673",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.to_csv('df5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223dad1a",
   "metadata": {},
   "source": [
    "I will select all of the features in our updated df except the target and PlayerId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925ec25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"AAV\"\n",
    "features = [col for col in df5.columns if col != target and col != \"PlayerId\" and col!= \"Name\" and col!= \"year\"]\n",
    "X, y = df5[features], df5[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e22b1d",
   "metadata": {},
   "source": [
    "## Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166af592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce115d5",
   "metadata": {},
   "source": [
    "#### Distribution of y_train values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e540d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y_train, bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.title('Histogram of y_train')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.kdeplot(y_train, shade=True)\n",
    "plt.title('KDE of y_train')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c77c77",
   "metadata": {},
   "source": [
    "#### Distribution of test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baad653",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y_test, bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.title('Histogram of y_test')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.kdeplot(y_test, shade=True)\n",
    "plt.title('KDE of y_test')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ccf8a0",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ddab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.coef_, linear_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e2e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_y_pred = linear_model.predict(X_test)\n",
    "\n",
    "results_df = X_test.copy()\n",
    "results_df[\"y_real\"] = y_test\n",
    "results_df[\"y_pred\"] = linear_y_pred\n",
    "results_df[\"err\"] = results_df[\"y_real\"] - results_df[\"y_pred\"]\n",
    "results_df[\"%_err\"] = results_df[\"err\"] / results_df[\"y_real\"] * 100\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15204917",
   "metadata": {},
   "source": [
    "### Evaluation Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c0f1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "print(f\"RMSE: {mean_squared_error(y_test, linear_y_pred)**0.5}\")\n",
    "print(f\"MAPE: {mean_absolute_percentage_error(y_test, linear_y_pred)}\")\n",
    "print(f\"R^2: {r2_score(y_test, linear_y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee5bb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_values(values, bin_size):\n",
    "    return np.floor(values / bin_size).astype(int)\n",
    "\n",
    "bin_size = 5\n",
    "y_test_binned = bin_values(y_test, bin_size)\n",
    "linear_y_pred_binned = bin_values(linear_y_pred, bin_size)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_binned, linear_y_pred_binned)\n",
    "\n",
    "bins = range(conf_matrix.shape[0])\n",
    "bin_labels = [f'{i*bin_size}-{(i+1)*bin_size}' for i in bins]\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=bin_labels, columns=bin_labels)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.title('Confusion Matrix (Binned Ranges)')\n",
    "plt.xlabel('Predicted Ranges')\n",
    "plt.ylabel('Actual Ranges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ab7399",
   "metadata": {},
   "source": [
    "#### Correction for negative predictions (Linear Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5a173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_values_exist = (results_df[\"y_pred\"] < 0).any()\n",
    "\n",
    "if negative_values_exist:\n",
    "    print(\"There are negative values in the 'y_pred' column.\")\n",
    "else:\n",
    "    print(\"There are no negative values in the 'y_pred' column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cca934",
   "metadata": {},
   "source": [
    "If our target variable (AAV) is strictly positive, we might transform the target variable before training the model and then inverse transform the predictions. A common transformation for strictly positive data is the logarithm.\n",
    "\n",
    "Step-by-Step Implementation\n",
    "\n",
    "    Log Transformation: \n",
    "        We apply a logarithmic transformation to the target variable (y_train). This maps the target values from the positive domain to the real number domain, where the regression model can better capture the relationships without producing negative predictions.\n",
    "\n",
    "    Training the Model: \n",
    "        Train the regression model using the transformed target variable.\n",
    "\n",
    "    Prediction: \n",
    "        Make predictions using the trained model on the test set.\n",
    "\n",
    "    Inverse Transformation: \n",
    "        Apply the exponential function to the predicted values to transform them back to the original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a288b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()\n",
    "\n",
    "y_train_log = np.log(y_train)\n",
    "linear_model.fit(X_train, y_train_log)\n",
    "\n",
    "linear_y_pred_log = linear_model.predict(X_test)\n",
    "\n",
    "\n",
    "linear_y_pred = np.exp(linear_y_pred_log)\n",
    "\n",
    "results_df = X_test.copy()\n",
    "results_df[\"y_real\"] = y_test\n",
    "results_df[\"y_pred\"] = linear_y_pred\n",
    "results_df[\"err\"] = results_df[\"y_real\"] - results_df[\"y_pred\"]\n",
    "results_df[\"%_err\"] = results_df[\"err\"] / results_df[\"y_real\"] * 100\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509ec99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE: {mean_squared_error(y_test, linear_y_pred)**0.5}\")\n",
    "print(f\"MAPE: {mean_absolute_percentage_error(y_test, linear_y_pred)}\")\n",
    "print(f\"R^2: {r2_score(y_test, linear_y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6695d7b",
   "metadata": {},
   "source": [
    "The model improves drastically in terms of average deviation (MAPE). It went from predicting with an error of 100% on average, to 50%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f778c8f1",
   "metadata": {},
   "source": [
    "### Evaluating per ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4242da22",
   "metadata": {},
   "source": [
    "#### Testing 0-5 range values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d46710",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_min, range_max = 0, 5\n",
    "\n",
    "def in_range(y_real, y_pred, range_min, range_max):\n",
    "    return range_min <= y_real <= range_max and range_min <= y_pred <= range_max\n",
    "\n",
    "results_df['y_real_in_range'] = results_df['y_real'].apply(lambda y: range_min <= y <= range_max)\n",
    "\n",
    "results_df['in_range'] = results_df.apply(lambda row: in_range(row['y_real'], row['y_pred'], range_min, range_max), axis=1)\n",
    "\n",
    "total_y_real_in_range = results_df['y_real_in_range'].sum()\n",
    "\n",
    "correct_predictions = results_df['in_range'].sum()\n",
    "\n",
    "print(f'Number of y_real values in the range {range_min}-{range_max}: {total_y_real_in_range}')\n",
    "print(f'Number of correct predictions in the range {range_min}-{range_max}: {correct_predictions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e22f1e",
   "metadata": {},
   "source": [
    "#### All ranges visualized with a bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ee1428",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = [ (0, 5), (5, 10), (10, 15), (15, 20), (20, 25), (25, 30), (30, 35), (35, 40), (40, float('inf'))]\n",
    "\n",
    "def in_range(y_real, y_pred, range_min, range_max):\n",
    "    return range_min <= y_real <= range_max and range_min <= y_pred <= range_max\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for range_min, range_max in ranges:\n",
    "    results_df['y_real_in_range'] = results_df['y_real'].apply(lambda y: range_min <= y <= range_max)\n",
    "    results_df['in_range'] = results_df.apply(lambda row: in_range(row['y_real'], row['y_pred'], range_min, range_max), axis=1)\n",
    "    total_y_real_in_range = results_df['y_real_in_range'].sum()\n",
    "    \n",
    "    correct_predictions = results_df['in_range'].sum()\n",
    "\n",
    "    results_list.append({\n",
    "        'Range': f'{range_min}-{range_max}' if range_max != float('inf') else f'{range_min}+',\n",
    "        'Total Real in range': total_y_real_in_range,\n",
    "        'Correct_Predictions': correct_predictions\n",
    "    })\n",
    "\n",
    "results_summary = pd.DataFrame(results_list)\n",
    "\n",
    "print(results_summary)\n",
    "\n",
    "results_summary.set_index('Range').plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Number of Real values and Correct Predictions by Range')\n",
    "plt.xlabel('Range')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5393989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_values(values, bin_size):\n",
    "    return np.floor(values / bin_size).astype(int)\n",
    "\n",
    "bin_size = 5\n",
    "y_test_binned = bin_values(y_test, bin_size)\n",
    "linear_y_pred_binned = bin_values(linear_y_pred, bin_size)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_binned, linear_y_pred_binned)\n",
    "\n",
    "bins = range(conf_matrix.shape[0])\n",
    "bin_labels = [f'{i*bin_size}-{(i+1)*bin_size}' for i in bins]\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=bin_labels, columns=bin_labels)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.title('Confusion Matrix (Binned Ranges)')\n",
    "plt.xlabel('Predicted Ranges')\n",
    "plt.ylabel('Actual Ranges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347323e2",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03147792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb_model = XGBRegressor()\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "results_df_xgb = X_test.copy()\n",
    "results_df_xgb[\"y_real\"] = y_test\n",
    "results_df_xgb[\"y_pred\"] = xgb_y_pred\n",
    "results_df_xgb[\"err\"] = results_df_xgb[\"y_real\"] - results_df_xgb[\"y_pred\"]\n",
    "results_df_xgb[\"%_err\"] = results_df_xgb[\"err\"] / results_df_xgb[\"y_real\"] * 100\n",
    "\n",
    "\n",
    "results_df_xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea20a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE: {mean_squared_error(y_test, xgb_y_pred)**0.5}\")\n",
    "print(f\"MAPE: {mean_absolute_percentage_error(y_test, xgb_y_pred)}\")\n",
    "print(f\"R^2: {r2_score(y_test, xgb_y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144a364e",
   "metadata": {},
   "source": [
    "The XGBoost model performed better in terms of the average deviation (MAPE), but it was still missing by almost 100% every time. The RMSE rose to 5.18 from 4.63 on average in the LM. The MAPE, like I said, decreased slightly from 106% to 92% on average. And R^2 score dropped by 10 percent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14db670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_values(values, bin_size):\n",
    "    return np.floor(values / bin_size).astype(int)\n",
    "\n",
    "bin_size = 5\n",
    "y_test_binned = bin_values(y_test, bin_size)\n",
    "xgb_y_pred_binned = bin_values(xgb_y_pred, bin_size)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_binned, xgb_y_pred_binned)\n",
    "\n",
    "bins = range(conf_matrix.shape[0])\n",
    "bin_labels = [f'{i*bin_size}-{(i+1)*bin_size}' for i in bins]\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=bin_labels, columns=bin_labels)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.title('Confusion Matrix (Binned Ranges)')\n",
    "plt.xlabel('Predicted Ranges')\n",
    "plt.ylabel('Actual Ranges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2440e1df",
   "metadata": {},
   "source": [
    "#### Correction for negative predictions (XGBoost Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55988c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_values_exist = (results_df_xgb[\"y_pred\"] < 0).any()\n",
    "\n",
    "if negative_values_exist:\n",
    "    print(\"There are negative values in the 'y_pred' column.\")\n",
    "else:\n",
    "    print(\"There are no negative values in the 'y_pred' column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef87ee",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16474ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_model = Ridge()\n",
    "\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "ridge_y_pred = ridge_model.predict(X_test)\n",
    "\n",
    "results_df_ridge = X_test.copy()\n",
    "results_df_ridge[\"y_real\"] = y_test\n",
    "results_df_ridge[\"y_pred\"] = ridge_y_pred\n",
    "results_df_ridge[\"err\"] = results_df_ridge[\"y_real\"] - results_df_ridge[\"y_pred\"]\n",
    "results_df_ridge[\"%_err\"] = results_df_ridge[\"err\"] / results_df_ridge[\"y_real\"] * 100\n",
    "\n",
    "results_df_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd458578",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE: {mean_squared_error(y_test, ridge_y_pred)**0.5}\")\n",
    "print(f\"MAPE: {mean_absolute_percentage_error(y_test, ridge_y_pred)}\")\n",
    "print(f\"R^2: {r2_score(y_test, ridge_y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399059f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_values(values, bin_size):\n",
    "    return np.floor(values / bin_size).astype(int)\n",
    "\n",
    "bin_size = 5\n",
    "y_test_binned = bin_values(y_test, bin_size)\n",
    "ridge_y_pred_binned = bin_values(ridge_y_pred, bin_size)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_binned, ridge_y_pred_binned)\n",
    "\n",
    "bins = range(conf_matrix.shape[0])\n",
    "bin_labels = [f'{i*bin_size}-{(i+1)*bin_size}' for i in bins]\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=bin_labels, columns=bin_labels)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.title('Confusion Matrix (Binned Ranges)')\n",
    "plt.xlabel('Predicted Ranges')\n",
    "plt.ylabel('Actual Ranges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e0906",
   "metadata": {},
   "source": [
    "##### Correction for negative predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1161b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_values_exist = (results_df_ridge[\"y_pred\"] < 0).any()\n",
    "\n",
    "if negative_values_exist:\n",
    "    print(\"There are negative values in the 'y_pred' column.\")\n",
    "else:\n",
    "    print(\"There are no negative values in the 'y_pred' column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7bd435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "y_train_log = np.log(y_train)\n",
    "\n",
    "\n",
    "ridge_model = Ridge()\n",
    "ridge_model.fit(X_train, y_train_log)\n",
    "\n",
    "\n",
    "ridge_y_pred_log = ridge_model.predict(X_test)\n",
    "\n",
    "\n",
    "ridge_y_pred = np.exp(ridge_y_pred_log)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, ridge_y_pred))\n",
    "r2 = r2_score(y_test, ridge_y_pred)\n",
    "mape = np.mean(np.abs((y_test - ridge_y_pred) / y_test)) \n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAPE: {mape}\")\n",
    "print(f\"R^2: {r2}\")\n",
    "\n",
    "\n",
    "results_df_ridge = X_test.copy()\n",
    "results_df_ridge[\"y_real\"] = y_test\n",
    "results_df_ridge[\"y_pred\"] = ridge_y_pred\n",
    "results_df_ridge[\"err\"] = results_df_ridge[\"y_real\"] - results_df_ridge[\"y_pred\"]\n",
    "results_df_ridge[\"%_err\"] = results_df_ridge[\"err\"] / results_df_ridge[\"y_real\"] * 100\n",
    "\n",
    "results_df_ridge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e0037",
   "metadata": {},
   "source": [
    "#### Testing 0-5 range values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5b1b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_min, range_max = 0, 5\n",
    "\n",
    "def in_range(y_real, y_pred, range_min, range_max):\n",
    "    return range_min <= y_real <= range_max and range_min <= y_pred <= range_max\n",
    "\n",
    "results_df_ridge['y_real_in_range'] = results_df_ridge['y_real'].apply(lambda y: range_min <= y <= range_max)\n",
    "\n",
    "results_df_ridge['in_range'] = results_df_ridge.apply(lambda row: in_range(row['y_real'], row['y_pred'], range_min, range_max), axis=1)\n",
    "\n",
    "total_y_real_in_range = results_df_ridge['y_real_in_range'].sum()\n",
    "\n",
    "correct_predictions = results_df_ridge['in_range'].sum()\n",
    "\n",
    "print(f'Number of Real values in the range {range_min}-{range_max}: {total_y_real_in_range}')\n",
    "print(f'Number of correct predictions in the range {range_min}-{range_max}: {correct_predictions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7029509c",
   "metadata": {},
   "source": [
    "#### All ranges visualized with a bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0526d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_range(y_real, y_pred, range_min, range_max):\n",
    "    return range_min <= y_real <= range_max and range_min <= y_pred <= range_max\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for range_min, range_max in ranges:\n",
    "    results_df_ridge['y_real_in_range'] = results_df_ridge['y_real'].apply(lambda y: range_min <= y <= range_max)\n",
    "    results_df_ridge['in_range'] = results_df_ridge.apply(lambda row: in_range(row['y_real'], row['y_pred'], range_min, range_max), axis=1)\n",
    "    total_y_real_in_range = results_df_ridge['y_real_in_range'].sum()\n",
    "    \n",
    "    correct_predictions = results_df_ridge['in_range'].sum()\n",
    "\n",
    "    results_list.append({\n",
    "        'Range': f'{range_min}-{range_max}' if range_max != float('inf') else f'{range_min}+',\n",
    "        'Total Real in range': total_y_real_in_range,\n",
    "        'Correct Predictions': correct_predictions\n",
    "    })\n",
    "\n",
    "results_summary = pd.DataFrame(results_list)\n",
    "\n",
    "print(results_summary)\n",
    "\n",
    "results_summary.set_index('Range').plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Number of Real values and Correct Predictions by Range')\n",
    "plt.xlabel('Range')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61acc622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_values(values, bin_size):\n",
    "    return np.floor(values / bin_size).astype(int)\n",
    "\n",
    "bin_size = 5\n",
    "y_test_binned = bin_values(y_test, bin_size)\n",
    "ridge_y_pred_binned = bin_values(ridge_y_pred, bin_size)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_binned, ridge_y_pred_binned)\n",
    "\n",
    "bins = range(conf_matrix.shape[0])\n",
    "bin_labels = [f'{i*bin_size}-{(i+1)*bin_size}' for i in bins]\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=bin_labels, columns=bin_labels)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.title('Confusion Matrix (Binned Ranges)')\n",
    "plt.xlabel('Predicted Ranges')\n",
    "plt.ylabel('Actual Ranges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e8b089",
   "metadata": {},
   "source": [
    "#### Min Max Scale for Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bd62d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_log = np.log(y_train)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "ridge_model = Ridge()\n",
    "ridge_model.fit(X_train_scaled, y_train_log)\n",
    "\n",
    "ridge_y_pred_log = ridge_model.predict(X_test_scaled)\n",
    "\n",
    "ridge_y_pred = np.exp(ridge_y_pred_log)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, ridge_y_pred))\n",
    "r2 = r2_score(y_test, ridge_y_pred)\n",
    "mape = np.mean(np.abs((y_test - ridge_y_pred) / y_test))\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAPE: {mape}\")\n",
    "print(f\"R^2: {r2}\")\n",
    "\n",
    "results_df_ridge = X_test.copy()\n",
    "results_df_ridge[\"y_real\"] = y_test\n",
    "results_df_ridge[\"y_pred\"] = ridge_y_pred\n",
    "results_df_ridge[\"err\"] = results_df_ridge[\"y_real\"] - results_df_ridge[\"y_pred\"]\n",
    "results_df_ridge[\"%_err\"] = results_df_ridge[\"err\"] / results_df_ridge[\"y_real\"] * 100\n",
    "\n",
    "results_df_ridge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff9900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_values(values, bin_size):\n",
    "    return np.floor(values / bin_size).astype(int)\n",
    "\n",
    "bin_size = 5\n",
    "y_test_binned = bin_values(y_test, bin_size)\n",
    "ridge_y_pred_binned = bin_values(ridge_y_pred, bin_size)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_binned, ridge_y_pred_binned)\n",
    "\n",
    "bins = range(conf_matrix.shape[0])\n",
    "bin_labels = [f'{i*bin_size}-{(i+1)*bin_size-1}' for i in bins]\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=bin_labels, columns=bin_labels)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.title('Confusion Matrix (Binned Ranges)')\n",
    "plt.xlabel('Predicted Ranges')\n",
    "plt.ylabel('Actual Ranges')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
